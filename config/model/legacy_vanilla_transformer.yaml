model_dim: 64
feedforward_dim: 128
activation: gelu
num_heads: 8
num_layers: 4
dropout: 0.1

# input/output dimensions
tracker_track_dim: ${eval:'len(${feature_set.tracker_track})'}
segment_dim: ${eval:'len(${feature_set.dt_segment})'}
hit_dim: ${eval:'len(${feature_set.rpc_hit})'}
output_dim: ${exp.target_dim}

module:
  _target_: deepmuonreco.nn.LegacyVanillaTransformerModel

  track_dim: ${..tracker_track_dim}
  segment_dim: ${..segment_dim}
  hit_dim: ${..hit_dim}
  output_dim: ${..output_dim}

  model_dim: ${..model_dim} # Dimension of the model embeddings
  feedforward_dim: ${..feedforward_dim} # Dimension of the feed-forward network
  activation: ${..activation} # Activation function (e.g., 'gelu', 'relu')
  num_heads: ${..num_heads} # Number of heads for multi-head attention
  num_layers: ${..num_layers} # Number of transformer layers
  dropout: ${..dropout} # Dropout probability

in_keys:
  - tracker_track_prep
  - [masks, tracker_track]
  - dt_segment_prep
  - [masks, dt_segment]
  - csc_segment_prep
  - [masks, csc_segment]
  - rpc_hit_prep
  - [masks, rpc_hit]
  - gem_hit_prep
  - [masks, gem_hit]

out_keys:
  - logits
