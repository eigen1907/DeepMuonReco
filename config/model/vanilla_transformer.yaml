model_dim: 64
feedforward_dim: 128
activation: gelu
num_heads: 8
num_layers: 4
dropout: 0.1

module:
  _target_: deepmuonreco.nn.VanillaTransformerModel

  track_dim: ${exp.track_dim}
  segment_dim: ${exp.segment_dim}
  hit_dim: ${exp.hit_dim}
  output_dim: ${exp.target_dim}

  model_dim: ${model.model_dim} # Dimension of the model embeddings
  feedforward_dim: ${model.feedforward_dim} # Dimension of the feed-forward network
  activation: ${model.activation} # Activation function (e.g., 'gelu', 'relu')
  num_heads: ${model.num_heads} # Number of heads for multi-head attention
  num_layers: ${model.num_layers} # Number of transformer layers
  dropout: ${model.dropout} # Dropout probability

in_keys:
  - track_prep
  - [pad_masks, track]
  - segment_prep
  - [pad_masks, segment]
  - rechit_prep
  - [pad_masks, rechit]

out_keys:
  - logits
